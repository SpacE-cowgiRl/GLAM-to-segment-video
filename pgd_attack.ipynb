{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T14:18:10.715948Z",
     "start_time": "2024-08-08T14:18:07.593407Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groundingdino.util.inference import Model\n",
    "from typing import List\n",
    "import os\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO, SAM\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n",
    "import rerun as rr\n",
    "#from track_utils import sample_points_from_masks\n",
    "#from video_utils import create_video_from_images\n",
    "import json\n",
    "import random\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0fbcd4-7e6e-4f56-84b9-d98f11a7a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supervision==0.22.0 in /home/lnt/anaconda3/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (6.0.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (3.9.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (0.7.1)\n",
      "Requirement already satisfied: pillow>=9.4 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (10.3.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.5.64 in /home/lnt/anaconda3/lib/python3.10/site-packages (from supervision==0.22.0) (4.8.0.74)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/lnt/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision==0.22.0) (3.0.9)\n",
      "Requirement already satisfied: six in /home/lnt/anaconda3/lib/python3.10/site-packages (from cycler>=0.10->matplotlib>=3.6.0->supervision==0.22.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install supervision==0.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16dacc27-e36f-4c02-94c6-ccd481e07af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfd96b6da01f270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T14:18:11.540043Z",
     "start_time": "2024-08-08T14:18:11.536970Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "for logger in loggers:\n",
    "    if \"transformers\" in logger.name.lower():\n",
    "        logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344c20cc98b8c491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T15:50:29.159016Z",
     "start_time": "2024-08-08T15:50:29.155027Z"
    }
   },
   "outputs": [],
   "source": [
    "#video_folder = [f\"{i}\" for i in range(1, 14)]\n",
    "#video_paths = [os.path.join(f'/mnt/data/Datasets/Innsbruk/test/output_video_part_{f}.mp4') for f in video_folder]\n",
    "#video_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75b9db-c83c-4f12-bcc6-5415c08823d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get PGD video\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义 PGD 攻击函数\n",
    "def pgd_attack(images, eps=0.3, alpha=0.01, iters=40):\n",
    "    images = images.clone().detach().requires_grad_(True)\n",
    "    delta = torch.zeros_like(images).uniform_(-eps, eps).to(images.device)\n",
    "    delta.requires_grad = True\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        outputs = images + delta  # 仅对图像进行扰动\n",
    "        loss = torch.mean(outputs)  # 使用简单的损失值，不涉及具体任务\n",
    "        loss.backward()\n",
    "        \n",
    "        grad = delta.grad.detach()\n",
    "        delta.data = delta + alpha * grad.sign()\n",
    "        delta.data = torch.clamp(delta, -eps, eps)\n",
    "        delta.grad.zero_()\n",
    "    \n",
    "    return (images + delta).detach()\n",
    "\n",
    "# 视频处理流程\n",
    "SOURCE_VIDEO_PATH = \"/home/lnt/PycharmProjects/sam/data/videocut.mp4\"\n",
    "output_path = \"/home/lnt/PycharmProjects/sam/data/attack.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# 逐帧处理视频\n",
    "for frame_number in tqdm(range(total_frames)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 将帧转换为RGB格式并调整尺寸以符合模型要求\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_rgb = cv2.resize(frame_rgb, (640, 640))  # 调整为640x640尺寸\n",
    "\n",
    "    # 将图像转换为Tensor格式\n",
    "    frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "\n",
    "    # 应用 PGD 攻击\n",
    "    frame_pgd_tensor = pgd_attack(frame_tensor)\n",
    "    frame_pgd = frame_pgd_tensor.squeeze().permute(1, 2, 0).numpy() * 255.0\n",
    "    frame_pgd = frame_pgd.astype(np.uint8)\n",
    "\n",
    "    # 将结果写入输出视频\n",
    "    out.write(cv2.cvtColor(frame_pgd, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# 关闭视频文件\n",
    "out.release()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f817a553-f40d-4b6d-9f48-72521db35bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLAMModel:\n",
    "    def __init__(self, grounding_dino_config_path, grounding_dino_checkpoint_path, sam_model_cfg, sam_checkpoint_path, prompt=None):\n",
    "        if prompt is None:\n",
    "            # prompt = ['pathways', 'trails', 'walkways', 'sidewalks', 'tracks', 'footpaths', 'routes', 'pedestrian paths', 'walking paths', 'lanes']\n",
    "            prompt = ['pavement', 'fence', 'cyclepath', 'trees', 'grasses', 'sidewalk', 'buildings', 'skies', 'streetlights']\n",
    "            '''\n",
    "            prompt = [\"street lamp\", \"street light\", \"lamppost\", \"road light\",\n",
    "                      \"trash can\", \"dustbin\", \"garbage bin\", \"waste container\",\n",
    "                      \"path\", \"pathway\", \"trail\", \"sidewalk\", \"footpath\", # , \"path edge\", \"trail border\"\n",
    "                      \"sculpture\", \"statue\", \"art installation\", \"monument\", # , \"public art\"\n",
    "                      \"pavement\", \"road\", \"lane\", # ,\"restroom\", \"toilet\", \"bathroom\", \"lavatory\", \"public restroom\", \"public toilet\"\n",
    "                      \"signpost\", \"tree\", \"bush\", \"fence\", # \"bench\", , \"street furniture\", \"bicycle rack\"\n",
    "                      \"swing\", \"slide\", # ,\"playground\",  \"merry-go-round\", \"sandbox\"\n",
    "                      \"grass\", \"lawn\", \"green area\", \"garden\", \"park\", \"meadow\",\n",
    "                      \"pond\", \"lake\", \"fountain\", \"stream\", \"river\" # , \"water feature\"\n",
    "                      ]\n",
    "            '''\n",
    "        self.prompt = prompt\n",
    "        self.grounding_dino_model = Model(model_config_path=grounding_dino_config_path, model_checkpoint_path=grounding_dino_checkpoint_path)\n",
    "        self.sam = build_sam2(sam_model_cfg, sam_checkpoint_path, device=\"cuda\")\n",
    "        self.sam_predictor = SAM2ImagePredictor(self.sam)\n",
    "        self.yolo = YOLO('yolov8x-seg.pt')\n",
    "        with open('class_descriptions.json', 'r', encoding='utf-8') as file:\n",
    "            self.class_names = json.load(file)\n",
    "\n",
    "        self.class_names += [{'id': 80+i, 'color': self.generate_random_color(), 'name': p} for i, p in enumerate(self.prompt)]\n",
    "        self.class_dict = {item['id']: item['name'] for item in self.class_names}\n",
    "        # self.dino_classes = 'pathways . trails . walkways'\n",
    "        self.dino_classes = self.enhance_class_name(self.prompt)\n",
    "        # self.dino_classes = str.join(' . ', self.prompt) + ' .'\n",
    "        self.dino_box_threshold = 0.35\n",
    "        self.dino_text_threshold = 0.25\n",
    "        self.class_descriptions = [rr.AnnotationInfo(id=cat[\"id\"], color=cat[\"color\"], label=cat[\"name\"]) for cat in self.class_names]\n",
    "        self.yolo_classes = [0, 1, 2, 3, 5, 7, 9, 11, 30]  # [0, 1, 2, 3, 5, 7, 9, 10, 11, 13, 14, 15, 16, 56, 60, 67]\n",
    "        self.persist = []\n",
    "        self.video_outs = dict()\n",
    "        \n",
    "    @staticmethod\n",
    "    def enhance_class_name(class_names: List[str]) -> List[str]:\n",
    "        return [f\"{class_name}\" for class_name in class_names]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random_color():\n",
    "        r = random.randint(0, 255)\n",
    "        g = random.randint(0, 255)\n",
    "        b = random.randint(0, 255)\n",
    "        return r, g, b\n",
    "    \n",
    "    def add_dino_class(self, _phrase):\n",
    "        _class_id = max(self.class_dict.keys()) + 1\n",
    "        self.class_names.append({'id': _class_id, 'color': self.generate_random_color(), 'name': _phrase})\n",
    "        self.class_dict = {item['id']: item['name'] for item in self.class_names}\n",
    "        self.class_descriptions = [rr.AnnotationInfo(id=cat[\"id\"], color=cat[\"color\"], label=cat[\"name\"]) for cat in self.class_names]\n",
    "        return _class_id\n",
    "    \n",
    "    def dino_id_to_class_name(self, dino_id):\n",
    "        return self.class_dict[dino_id]\n",
    "    \n",
    "    def phrases2classes(self, phrases: List[str]) -> (np.ndarray, bool):\n",
    "        class_ids = []\n",
    "        ret = False\n",
    "        for phrase in phrases:\n",
    "            if phrase in self.class_dict.values():\n",
    "                for k, v in self.class_dict.items():\n",
    "                    if v == phrase:\n",
    "                        class_ids.append(k)\n",
    "            else:\n",
    "                _class_id = self.add_dino_class(phrase)\n",
    "                class_ids.append(_class_id)\n",
    "                ret = True\n",
    "        return np.array(class_ids), ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8179962bc29d280d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T15:50:35.087405Z",
     "start_time": "2024-08-08T15:50:35.084035Z"
    }
   },
   "outputs": [],
   "source": [
    "GROUNDING_DINO_CONFIG_PATH = os.path.join('../', \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join('../', \"weights\", \"groundingdino_swint_ogc.pth\")\n",
    "SAM_CHECKPOINT_PATH = os.path.join(\"/home/lnt/PycharmProjects/sam/weights/sam2_hiera_large.pt\")\n",
    "SAM_MODEL_CFG = \"sam2_hiera_l.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccb8722-c28e-460f-be77-695c9dc35b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1817/1817 [21:53<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Frame          IoU         Dice\n",
      "count  1817.000000  1817.000000  1817.000000\n",
      "mean    908.000000     0.997218     0.009373\n",
      "std     524.667037     0.003040     0.001334\n",
      "min       0.000000     0.982170     0.006852\n",
      "25%     454.000000     0.996004     0.008518\n",
      "50%     908.000000     0.998290     0.009064\n",
      "75%    1362.000000     0.999357     0.009925\n",
      "max    1816.000000     0.999992     0.017289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 计算 IoU 和 Dice 指数\n",
    "def calculate_iou(pred, target):\n",
    "    intersection = np.logical_and(pred, target).sum()\n",
    "    union = np.logical_or(pred, target).sum()\n",
    "    return intersection / union if union != 0 else 1.0\n",
    "\n",
    "def calculate_dice(pred, target):\n",
    "    intersection = np.logical_and(pred, target).sum()\n",
    "    return 2 * intersection / (pred.sum() + target.sum()) if (pred.sum() + target.sum()) != 0 else 1.0\n",
    "\n",
    "# 初始化GLAM模型\n",
    "glam_model = GLAMModel(\n",
    "    grounding_dino_config_path=GROUNDING_DINO_CONFIG_PATH, \n",
    "    grounding_dino_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH, \n",
    "    sam_model_cfg=SAM_MODEL_CFG, \n",
    "    sam_checkpoint_path=SAM_CHECKPOINT_PATH\n",
    ")\n",
    "\n",
    "# 视频路径\n",
    "video1_path = \"/home/lnt/PycharmProjects/sam/data/videocut.mp4\"\n",
    "video2_path = \"/home/lnt/PycharmProjects/sam/data/attack.mp4\"\n",
    "\n",
    "# 打开两个视频\n",
    "cap1 = cv2.VideoCapture(video1_path)\n",
    "cap2 = cv2.VideoCapture(video2_path)\n",
    "\n",
    "# 获取视频属性\n",
    "fps = int(cap1.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# 输出视频路径\n",
    "output_video_path = \"//home/lnt/PycharmProjects/sam/data/comparison.mp4\"\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "# 初始化结果存储\n",
    "results = {\n",
    "    \"Frame\": [],\n",
    "    \"IoU\": [],\n",
    "    \"Dice\": []\n",
    "}\n",
    "\n",
    "# 获取视频总帧数\n",
    "total_frames = min(int(cap1.get(cv2.CAP_PROP_FRAME_COUNT)), int(cap2.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "# 逐帧处理两个视频\n",
    "for frame_number in tqdm(range(total_frames)):\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "    \n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "    \n",
    "    # 调整帧尺寸以匹配\n",
    "    if frame1.shape != frame2.shape:\n",
    "        frame2 = cv2.resize(frame2, (frame1.shape[1], frame1.shape[0]))\n",
    "        \n",
    "    # 对两个帧分别进行分割\n",
    "    def segment_frame(frame):\n",
    "        yolo_results = glam_model.yolo.track(frame, verbose=False, conf=0.6, persist=True, retina_masks=True, classes=glam_model.yolo_classes)[0]\n",
    "        dino_results, phrases = glam_model.grounding_dino_model.predict_with_caption(\n",
    "            image=frame,\n",
    "            caption=str.join(' . ', glam_model.prompt),\n",
    "            box_threshold=glam_model.dino_box_threshold,\n",
    "            text_threshold=glam_model.dino_text_threshold\n",
    "        )\n",
    "        dino_results.class_id, ret = glam_model.phrases2classes(phrases)\n",
    "        \n",
    "        glam_model.sam_predictor.set_image(frame)\n",
    "        \n",
    "        # 初始化 annotated_frame\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        for _box, _cls_id, _confidence, _phrase in zip(dino_results.xyxy, dino_results.class_id, dino_results.confidence, phrases):\n",
    "            _masks, _scores, _logits = glam_model.sam_predictor.predict(\n",
    "                box=_box,\n",
    "                multimask_output=True\n",
    "            )\n",
    "            _index = np.argmax(_scores)\n",
    "            _mask = _masks[_index]\n",
    "            _mask = _mask.astype('bool')\n",
    "            mask_result = sv.Detections(np.array([_box]), np.array([_mask]), np.array([_confidence]), np.array([_cls_id]))\n",
    "\n",
    "            mask_annotator = sv.MaskAnnotator()\n",
    "            box_annotator = sv.BoxAnnotator()\n",
    "            label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
    "            annotated_frame = mask_annotator.annotate(scene=frame, detections=mask_result)\n",
    "            annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=mask_result)\n",
    "            annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=mask_result, labels=[_phrase])\n",
    "        return annotated_frame\n",
    "\n",
    "    segmented_frame1 = segment_frame(frame1)\n",
    "    segmented_frame2 = segment_frame(frame2)\n",
    "    \n",
    "    # 计算 IoU 和 Dice 系数\n",
    "    iou = calculate_iou(segmented_frame1, segmented_frame2)\n",
    "    dice = calculate_dice(segmented_frame1, segmented_frame2)\n",
    "    \n",
    "    # 保存计算结果\n",
    "    results[\"Frame\"].append(frame_number)\n",
    "    results[\"IoU\"].append(iou)\n",
    "    results[\"Dice\"].append(dice)\n",
    "    \n",
    "    # 拼接两个分割后的帧用于展示\n",
    "    comparison_frame = np.hstack((segmented_frame1, segmented_frame2))\n",
    "    \n",
    "    # 写入对比视频\n",
    "    out.write(comparison_frame)\n",
    "\n",
    "# 释放视频资源\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "out.release()\n",
    "\n",
    "# 将结果存储为DataFrame并保存为CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"comparison_results.csv\", index=False)\n",
    "\n",
    "# 打印结果概览\n",
    "print(df_results.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
